- https://github.com/NVIDIA/Megatron-LM
- https://youtu.be/S27pHKBEp30?t=1392 transfer learning bert


* Install on gentoo

  #+begin_example
sudo emerge python-regex

cd ~/src
git clone https://github.com/NVIDIA/apex
cd apex
sudo eselect gcc set 1 # switch to gcc 8.4
. /etc/profile
pip install --user -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./

cd ~/src
git clone https://github.com/NVIDIA/Megatron-LM
  #+end_example


- pre-trained model from here (i downloaded this with browser, not ngc):
 https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m/version

#+begin_example
martin@labolg ~/stage/cl-py-generator/example/14_megatron_gpt/source/data/model $ sum latest_checkpointed_iteration.txt 
34929     1
martin@labolg ~/stage/cl-py-generator/example/14_megatron_gpt/source/data/model $ sum release/mp_rank_00/model_optim_rng.pt 
65317 693172


extract again after running python script:
martin@labolg ~/stage/cl-py-generator/example/14_megatron_gpt/source/data/model $ sum latest_checkpointed_iteration.txt 
34929     1
martin@labolg ~/stage/cl-py-generator/example/14_megatron_gpt/source/data/model $ sum release/mp_rank_00/model_optim_rng.pt 
65317 693172


#+end_example


* Example run

- https://github.com/NVIDIA/Megatron-LM#gpt-2-text-generation

#+begin_example
export MERGE_FILE=~/stage/cl-py-generator/example/14_megatron_gpt/source/data/gpt2-merges.txt
export VOCAB_FILE=~/stage/cl-py-generator/example/14_megatron_gpt/source/data/gpt2-vocab.json
export CHECKPOINT_PATH=/home/martin/stage/cl-py-generator/example/14_megatron_gpt/source/data/model
 ~/src/Megatron-LM $ python -i tools/generate_samples_gpt2.py --model-parallel-size 1 --num-layers 24 --hidden-size 1024 --load $CHECKPOINT_PATH --num-attention-heads 16        --max-position-embeddings 1024          --fp16        --batch-size 2        --seq-length 1024        --out-seq-length 1024        --temperature 1.0        --vocab-file $VOCAB_FILE        --merge-file $MERGE_FILE        --genfile unconditional_samples.json        --num-samples 2        --top_p 0.9        --recompute

#+end_example

don't use this parameter
#+begin_example
 --tokenizer-type GPT2BPETokenizer 
#+end_example
 

{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "#|default_exp p00_titanic_from_scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# this file is based on https://github.com/fastai/course22/blob/master/05-linear-model-and-neural-net-from-scratch.ipynb\n",
        "import os\n",
        "import time\n",
        "import pathlib\n",
        "import argparse\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "start_time=time.time()\n",
        "debug=True\n",
        "_code_git_version=\"571341e93a8741a422d7240a5d30c2df5980142c\"\n",
        "_code_repository=\"https://github.com/plops/cl-py-generator/tree/master/example/96_colab_fastai/source/\"\n",
        "_code_generation_time=\"18:32:33 of Monday, 2022-08-29 (GMT+1)\"\n",
        "start_time=time.time()\n",
        "debug=True\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "parser=argparse.ArgumentParser()\n",
        "parser.add_argument(\"-v\", \"--verbose\", help=\"enable verbose output\", action=\"store_true\")\n",
        "args=parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "export1(# i want to run this on google colab. annoyingly i can't seem to access the titanic.zip file. it seems to be necessary to supply some kaggle login information in a json file. rather than doing this i downloaded the titanic.zip file into my google drive\n",
        ", import google.colab.drive\n",
        ", google.colab.drive.mount(\"/content/drive\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "path=pathlib.Path(\"titanic\")\n",
        "if ( not(path.exists()) ):\n",
        "    import zipfile\n",
        "    zipfile.ZipFile(f\"/content/drive/MyDrive/{path}.zip\").extractall(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "path=pathlib.Path(\"titanic\")\n",
        "if ( not(path.exists()) ):\n",
        "    import zipfile\n",
        "    import kaggle\n",
        "    kaggle.api.competition_download_cli(str(path))\n",
        "    zipfile.ZipFile(f\"{path}.zip\").extractall(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "line_char_width=140\n",
        "np.set_printoptions(linewidth=line_char_width)\n",
        "torch.set_printoptions(linewidth=line_char_width, sci_mode=False, edgeitems=7)\n",
        "pd.set_option(\"display.width\", line_char_width)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "df=pd.read_csv(((path)/(\"train.csv\")))\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "modes=df.mode().iloc[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "df.fillna(modes, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.describe(include=(np.number,))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.Fare.hist()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "df[\"LogFare\"]=np.log(((1)+(df.Fare)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# histogram of logarithm of prices no longer shows the 'long' tail\n",
        "df.LogFare.hist()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# look at the three values that are in passenger class. more details about the dataset are here: https://www.kaggle.com/competitions/titanic/data\n",
        "pclasses=sorted(df.Pclass.unique())\n",
        "pclasses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# look at columns with non-numeric values\n",
        "df.describe(include=[object])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# replace non-numeric values with numbers by introducing new columns (dummies). The dummy columns will be added to the dataframe df and the 3 original columns are dropped.\n",
        "# Cabin, Name and Ticket contain too many unique values for this approach to be useful\n",
        "df=pd.get_dummies(df, columns=[\"Sex\", \"Pclass\", \"Embarked\"])\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# look at the new dummy columns\n",
        "added_columns=[\"Sex_male\", \"Sex_female\", \"Pclass_1\", \"Pclass_2\", \"Pclass_3\", \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\"]\n",
        "df[added_columns].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# create dependent variable as tensor\n",
        "t_dep=tensor(df.Survived)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# independent variables are all continuous variables of interest and the newly created columns\n",
        "indep_columns=(([\"Age\", \"SibSp\", \"Parch\", \"LogFare\"])+(added_columns))\n",
        "t_indep=tensor(df[indep_columns].values, dtype=torch.float)\n",
        "t_indep\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "t_indep.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# set up linear model. first we calculate manually a single step for the loss of every row in the dataset. we start with a random coefficient in (-.5,.5) for each column of t_indep\n",
        "torch.manual_seed(442)\n",
        "n_coeffs=t_indep.shape[1]\n",
        "coeffs=((torch.rand(n_coeffs))-((0.50    )))\n",
        "coeffs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# our predictions are formed by multiplying a row with coefficients and summing them up. we don't need to introduce a bias (or intercept) term by introducing a column containing only ones. Such a 'one' is already present in each row in either the dummy column Sex_male or Sex_female.\n",
        "((t_indep)*(coeffs))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# we have a potential problem with the first column Age. Its values are bigger in average than the values in other columns\n",
        "# In the lecture Jeremy mentions two options to normalize Age I can think of two more methods: 1) divide by maximum or 2) subtract mean and divide by std 3) subtract median and divide by MAD 4) find lower 2 perscentile and upper 2 percentile increase the value gap by +/- 10% and use this interval to normalize the input values. In the book jeremy uses 1). 1) and 3) differ by how they handle outliers. The maximum will be influenced a lot by outliers. I would like to know if 3) is better than 1) for typical problems. I think that boils down to how big the training dataset is. Once it is big enough there may be always enough outliers to ensure even the maximum is stable.\n",
        "if ( True ):\n",
        "    # method 1)\n",
        "    vals, indices=t_indep.max(dim=0)\n",
        "    t_indep=((t_indep)/(vals))\n",
        "if ( False ):\n",
        "    # method 2)\n",
        "    means, indices1=t_indep.mean(dim=0)\n",
        "    stdts, indices2=t_indep.std(dim=0)\n",
        "    t_indep=((((t_indep)-(means)))/(stds))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create predictions by adding up the rows of the product\n",
        "preds=((t_indep)*(coeffs)).sum(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# look at first few\n",
        "preds[:10]\n",
        "# as the coefficents were random these predictions are no good\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# in order to improve the predictions modify the coefficients with gradient descent\n",
        "# define the loss as the average error between predictions and the dependent\n",
        "loss=torch.abs(((preds)-(t_dep))).mean()\n",
        "loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# using what we learned in the previous cells create functions to compute predictions and loss\n",
        "def calc_preds(coeffs=None, indeps=None):\n",
        "    return ((indeps)*(coeffs)).sum(axis=1)\n",
        "def calc_loss(coeffs=None, indeps=None, deps=None):\n",
        "    preds=calc_preds(coeffs=coeffs, indeps=indeps)\n",
        "    loss=torch.abs(((preds)-(deps))).mean()\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# perform a single 'epoch' of gradient descent manually\n",
        "# tell pytorch that we want to calculate the gradients for the coeffs object. the underscore indicates that the coeffs object will be modified in place\n",
        "coeffs.requires_grad_()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# compute the loss, pytorch will perform book keeping to compute gradients later\n",
        "loss=calc_loss(coeffs=coeffs, indeps=t_indep, deps=t_dep)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# compute gradient\n",
        "loss.backward()\n",
        "coeffs.grad\n",
        "# note that every call of backward() adds the gradients to grad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# calling the steps a second time will double the values in .grad\n",
        "loss=calc_loss(coeffs=coeffs, indeps=t_indep, deps=t_dep)\n",
        "loss.backward()\n",
        "coeffs.grad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# we can now perform a single gradient step. the loss should reduce\n",
        "loss=calc_loss(coeffs=coeffs, indeps=t_indep, deps=t_dep)\n",
        "loss.backward()\n",
        "with torch.no_grad():\n",
        "    coeffs.sub_(((coeffs.grad)*((0.10    ))))\n",
        "    coeffs.grad.zero_()\n",
        "    print(calc_loss(coeffs=coeffs, indeps=t_indep, deps=t_dep))\n",
        "# a.sub_(b) subtracts the gradient from coeffs in place (a = a - b) and zero_ clears the gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# before we can perform training, we have to create a validation dataset\n",
        "# we do that in the same way as the fastai library does\n",
        "import fastai.data.transforms\n",
        "# get training (trn) and validation indices (val)\n",
        "trn, val=(fastai.data.transforms.RandomSplitter(seed=42))((df))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "trn_indep=t_indep[trn]\n",
        "val_indep=t_indep[val]\n",
        "trn_dep=t_dep[trn]\n",
        "val_dep=t_dep[val]\n",
        "len(trn_indep), len(val_indep)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# create 3 functions for the operations that were introduced in the previous cells\n",
        "def update_coeffs(coeffs=None, learning_rate=None):\n",
        "    coeffs.sub_(((coeffs.grad)*(learning_rate)))\n",
        "    coeffs.grad.zero_()\n",
        "def init_coeffs():\n",
        "    coeffs=((torch.rand(n_coeffs))-((0.50    )))\n",
        "    coeffs.requires_grad_()\n",
        "    return coeffs\n",
        "def one_epoch(coeffs=None, learning_rate=None):\n",
        "    loss=calc_loss(coeffs=coeffs, indeps=trn_indep, deps=trn_dep)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        update_coeffs(coeffs=coeffs, learning_rate=learning_rate)\n",
        "    print(f\"{loss:.3f}\", end=\"; \")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# now use these functions to train the model\n",
        "def train_model(epochs=30, learning_rate=(1.00e-2)):\n",
        "    torch.manual_seed(442)\n",
        "    coeffs=init_coeffs()\n",
        "    for i in range(epochs):\n",
        "        one_epoch(coeffs=coeffs, learning_rate=learning_rate)\n",
        "    return coeffs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# try training. the loss should decrease\n",
        "coeffs=train_model(epochs=18, learning_rate=(0.20    ))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

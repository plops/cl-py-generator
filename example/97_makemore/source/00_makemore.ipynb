{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "#|default_exp p00_makemore\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# this file is based on https://github.com/fastai/course22/blob/master/05-linear-model-and-neural-net-from-scratch.ipynb\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "import os\n",
        "import time\n",
        "import pathlib\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from matplotlib.pyplot import plot, imshow, tight_layout, xlabel, ylabel, title, subplot, subplot2grid, grid, text, legend, figure, gcf, xlim, ylim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "start_time=time.time()\n",
        "debug=True\n",
        "_code_git_version=\"2765409fac4cfde8033da38c45dfd6358b1bc86c\"\n",
        "_code_repository=\"https://github.com/plops/cl-py-generator/tree/master/example/97_makemore/source/\"\n",
        "_code_generation_time=\"14:23:00 of Friday, 2024-05-10 (GMT+1)\"\n",
        "start_time=time.time()\n",
        "debug=True\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "parser=argparse.ArgumentParser()\n",
        "parser.add_argument(\"-v\", \"--verbose\", help=\"enable verbose output\", action=\"store_true\")\n",
        "args=parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "words=open(\"/home/martin/stage/cl-py-generator/example/97_makemore/source/names.txt\", \"r\").read().splitlines()\n",
        "words[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "min(len(w) for w in words)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "max(len(w) for w in words)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# collect statistics for pairs of characters\n",
        "b={}\n",
        "for w in words:\n",
        "    chs=(([\"<S>\"])+(list(w))+([\"<E>\"]))\n",
        "    for bigram in zip(chs, chs[1:]):\n",
        "        b[bigram]=((b.get(bigram, 0))+(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# show statistics sorted by frequency\n",
        "sorted(b.items(), key=lambda kv: ((-1)*(kv[1])))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "character_set=sorted(list(set(\"\".join(words))))\n",
        "len(character_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "stoi={s:i+1 for i, s in enumerate(character_set)}\n",
        "stoi[\".\"]=0\n",
        "stoi\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# invert lookup\n",
        "itos={i:s for s, i in stoi.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# 2d array is more convenient\n",
        "number_tokens=len(stoi)\n",
        "N=torch.zeros((number_tokens,number_tokens,), dtype=torch.int32)\n",
        "for w in words:\n",
        "    chs=(([\".\"])+(list(w))+([\".\"]))\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1=stoi[ch1]\n",
        "        ix2=stoi[ch2]\n",
        "        N[ix1,ix2] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "imshow(N)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "figure(figsize=(16,16,))\n",
        "imshow(N, cmap=\"Blues\")\n",
        "for i in range(number_tokens):\n",
        "    for j in range(number_tokens):\n",
        "        chstr=((itos[i])+(itos[j]))\n",
        "        text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
        "        text(j, i, N[i,j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
        "plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "p=N[0].float()\n",
        "p=((p)/(p.sum()))\n",
        "p\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "g=torch.Generator().manual_seed(2147483647)\n",
        "p=torch.rand(3, generator=g)\n",
        "p=((p)/(p.sum()))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "torch.multinomial(p, num_samples=20, replacement=True, generator=g)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# https://pytorch.org/docs/stable/notes/broadcasting.html\n",
        "# adding one for model smoothing (we don't want zeros in the matrix)\n",
        "P=((N)+(1)).float()\n",
        "P=((P)/(P.sum(1, keepdim=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "log_likelihood=(0.    )\n",
        "n=0\n",
        "for w in [\"andrej\"]:\n",
        "    chs=(([\".\"])+(list(w))+([\".\"]))\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1=stoi[ch1]\n",
        "        ix2=stoi[ch2]\n",
        "        prob=P[ix1,ix2]\n",
        "        logprob=torch.log(prob)\n",
        "        log_likelihood += logprob\n",
        "        n += 1\n",
        "        # everything with probability higher than 4% is better than random\n",
        "        print(f\"{ch1}{ch2}: {prob:.4f} {logprob:.4f}\")\n",
        "print(f\"{log_likelihood=}\")\n",
        "# we are intersted in the product of all probabilities. this would be a small number so we look at the log\n",
        "# look at negative log_likelihood. the lowest we can get is 0\n",
        "nll=((-1)*(log_likelihood))\n",
        "print(f\"{nll=}\")\n",
        "# normalized log likelihood is what we use\n",
        "# normalized log likelihood of the training model is 2.454\n",
        "print(f\"{nll/n:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "xs=[]\n",
        "ys=[]\n",
        "for w in words[:1]:\n",
        "    chs=(([\".\"])+(list(w))+([\".\"]))\n",
        "    for ch1, ch2 in zip(chs, chs[1:]):\n",
        "        ix1=stoi[ch1]\n",
        "        ix2=stoi[ch2]\n",
        "        print(ch1, ch2)\n",
        "        xs.append(ix1)\n",
        "        ys.append(ix2)\n",
        "xs=tensor(xs)\n",
        "ys=tensor(ys)\n",
        "# encode integers with one-hot encoding\n",
        "xenc=F.one_hot(xs, num_classes=number_tokens).float()\n",
        "imshow(xenc)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "g=torch.Generator().manual_seed(2147483647)\n",
        "W=torch.randn((27,27,), generator=g, requires_grad=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# output is 5x27 @ 27x27 = 5x27\n",
        "# 27 neurons on 5 inputs\n",
        "# what is the firing rate of the 27 neurons on everyone of the 5 inputs\n",
        "# xenc @ W [3,13] indicates the firing rate of the 13 neuron for input 3. it is a dot-product of the 13th column of W with the input xenc\n",
        "# we exponentiate the numbers. negative numbers will be 0..1, positive numbers will be >1\n",
        "# we will interpret them as something equivalent to count (positive numbers). this is called logits. equivalent to the counts in the N matrix\n",
        "# converting logits to probabilities is called softmax\n",
        "# the closer values in W the closer the probabilities to equal\n",
        "# you can regularize by forcing W to be closer to zero ... W**2 term in loss\n",
        "logits=((xenc)@(W))\n",
        "counts=logits.exp()\n",
        "probs=((counts)/(counts.sum(1, keepdims=True)))\n",
        "probs\n",
        "loss=((-1)*(probs[torch.arange(5),ys].log().mean()))\n",
        "print(loss.item())\n",
        "# this is the forward pass\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# backward pass\n",
        "# clear gradient\n",
        "W.grad=None\n",
        "loss.backward()\n",
        "W.data += (((-0.10    ))*(W.grad))\n",
        "# gradient descent gives exactly the same model. sampling will be the same as the frequency counter\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "#|default_exp p01_makemore\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "import os\n",
        "import time\n",
        "import pathlib\n",
        "import tqdm\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from matplotlib.pyplot import plot, imshow, tight_layout, xlabel, ylabel, title, subplot, subplot2grid, grid, text, legend, figure, gcf, xlim, ylim\n",
        "from torch import linspace, randn, randint, tanh\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.verbose=True\n",
        "args=Args()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "start_time=time.time()\n",
        "debug=True\n",
        "_code_git_version=\"2765409fac4cfde8033da38c45dfd6358b1bc86c\"\n",
        "_code_repository=\"https://github.com/plops/cl-py-generator/tree/master/example/97_makemore/source/\"\n",
        "_code_generation_time=\"14:23:07 of Friday, 2024-05-10 (GMT+1)\"\n",
        "start_time=time.time()\n",
        "debug=True\n",
        "def lprint(msg, args):\n",
        "    if ( args.verbose ):\n",
        "        print(\"{} {}\".format(((time.time())-(start_time)), msg))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "parser=argparse.ArgumentParser()\n",
        "parser.add_argument(\"-v\", \"--verbose\", help=\"enable verbose output\", action=\"store_true\")\n",
        "args=parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "multi layer perceptron https://www.youtube.com/watch?v=TCH_1BHY58I \n",
        "reference bengio 2003 neural probabilistic language model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "words=open(\"/home/martin/stage/cl-py-generator/example/97_makemore/source/names.txt\", \"r\").read().splitlines()\n",
        "words[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "chars=sorted(list(set(\"\".join(words))))\n",
        "stoi={s:((i)+(1)) for i, s in enumerate(chars)}\n",
        "stoi[\".\"]=0\n",
        "itos={i:s for s, i in stoi.items()}\n",
        "print(itos)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "block_size=3\n",
        "X=[]\n",
        "Y=[]\n",
        "for w in words:\n",
        "    print(w)\n",
        "    context=(([0])*(block_size))\n",
        "    for ch in ((w)+(\".\")):\n",
        "        ix=stoi[ch]\n",
        "        X.append(context)\n",
        "        Y.append(ix)\n",
        "        context=((context[1:])+([ix]))\n",
        "X=tensor(X)\n",
        "Y=tensor(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "C=torch.randn((27,2,))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "((F.one_hot(tensor(5), num_classes=27).float())@(C))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "emb=C[X]\n",
        "W1=torch.randn((6,100,))\n",
        "b1=torch.randn((100,))\n",
        "W2=torch.randn((100,27,))\n",
        "b2=torch.randn((27,))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "h=torch.tanh(((((emb.view(-1, 6))@(W1)))+(b1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "logits=((((h)@(W2)))+(b2))\n",
        "counts=logits.exp()\n",
        "prob=((counts)/(counts.sum(1, keepdims=True)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# negative log-likelihood loss\n",
        "loss=((-1)*(prob[torch.arange(32),Y].log().mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "g=torch.Generator().manual_seed(2147483647)\n",
        "C=torch.randn((27,2,), generator=g)\n",
        "W1=torch.randn((6,100,))\n",
        "b1=torch.randn((100,))\n",
        "W2=torch.randn((100,27,))\n",
        "b2=torch.randn((27,))\n",
        "parameters=[C, W1, b1, W2, b2]\n",
        "for p in parameters:\n",
        "    p.requires_grad=True\n",
        "n_parameters=sum(p.nelement() for p in parameters)\n",
        "lprint(\"n_parameters={}\".format(n_parameters), args)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# forward\n",
        "emb=C[X]\n",
        "h=torch.tanh(((((emb.view(-1, 6))@(W1)))+(b1)))\n",
        "logits=((((h)@(W2)))+(b2))\n",
        "loss=F.cross_entropy(logits, Y)\n",
        "# backward pass\n",
        "# set gradients to zero\n",
        "for p in parameters:\n",
        "    p.grad=None\n",
        "loss.backward()\n",
        "# update\n",
        "for p in parameters:\n",
        "    p.data += (((-0.10    ))*(p.grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# learning loop\n",
        "for _ in range(10):\n",
        "    # forward\n",
        "    emb=C[X]\n",
        "    h=torch.tanh(((((emb.view(-1, 6))@(W1)))+(b1)))\n",
        "    logits=((((h)@(W2)))+(b2))\n",
        "    loss=F.cross_entropy(logits, Y)\n",
        "    lprint(\"loss.item()={}\".format(loss.item()), args)\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "        p.grad=None\n",
        "    loss.backward()\n",
        "    # update\n",
        "    for p in parameters:\n",
        "        p.data += (((-0.10    ))*(p.grad))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# learn with minibatch\n",
        "for _ in range(10_000):\n",
        "    ix=torch.randint(0, X.shape[0], (32,))\n",
        "    # forward\n",
        "    emb=C[X[ix]]\n",
        "    h=torch.tanh(((((emb.view(-1, 6))@(W1)))+(b1)))\n",
        "    logits=((((h)@(W2)))+(b2))\n",
        "    loss=F.cross_entropy(logits, Y[ix])\n",
        "    lprint(\"loss.item()={}\".format(loss.item()), args)\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "        p.grad=None\n",
        "    loss.backward()\n",
        "    # update\n",
        "    for p in parameters:\n",
        "        p.data += (((-0.10    ))*(p.grad))\n",
        "# report loss on entire data set\n",
        "emb=C[X]\n",
        "h=torch.tanh(((((emb.view(-1, 6))@(W1)))+(b1)))\n",
        "logits=((((h)@(W2)))+(b2))\n",
        "loss_full=F.cross_entropy(logits, Y)\n",
        "lprint(\"loss_full.item()={}\".format(loss_full.item()), args)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# find a good learning rate, start with a very small lr and increase exponentially\n",
        "lre=linspace(-3, 0, 1000)\n",
        "lre=((10)**(lre))\n",
        "lri=[]\n",
        "lossi=[]\n",
        "for lr in tqdm.tqdm(lre):\n",
        "    ix=torch.randint(0, X.shape[0], (32,))\n",
        "    # forward\n",
        "    emb=C[X[ix]]\n",
        "    h=torch.tanh(((((emb.view(-1, 6))@(W1)))+(b1)))\n",
        "    logits=((((h)@(W2)))+(b2))\n",
        "    loss=F.cross_entropy(logits, Y[ix])\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "        p.grad=None\n",
        "    loss.backward()\n",
        "    # update\n",
        "    for p in parameters:\n",
        "        p.data += ((-1)*(lr)*(p.grad))\n",
        "    # track stats\n",
        "    lri.append(lr)\n",
        "    lossi.append(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot(lre, lossi, alpha=(0.40    ))\n",
        "grid()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

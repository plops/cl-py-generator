- https://www.youtube.com/watch?v=PaCmpygFfXo
  The spelled-out intro to language modeling: building makemore
- https://github.com/karpathy/makemore
- neural network that creates strings that sound like names but are
  not names

- install dependencies
#+begin_example
pip3 install --user torch
# 777MB
#+end_example

- get dataset 30_000 names from a government website

#+begin_example
cd source
wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt
#+end_example 


* Part 3

- https://www.youtube.com/watch?v=P6sfmUTpUmc
- Building makemore Part 3: Activations & Gradients, BatchNorm

- when dealing with logits make sure that after initialization all the
  outputs are roughly equal in probability. you don't want to init the
  network to be confidently wrong. this will lead to a high loss and
  you have a hockey stick learning curve. you waste cycles during
  training to fix your bad initialization

- leaky relu doesn't make gradients zero. other activation functions
  have horizontal parts where the gradient disappears during
  backprop. this can lead to 'brain-dead' neurons that don't change in
  the training.

- look at histograms of pre-activations and activations
- also make binary plot (2d image) of saturated activations 
- to fix this, make sure during initialization that activations don't
  saturate too much (by scaling the weights and biases correctly)

- deeper networks are less forgiving to these issues with the
  initialization

- you want unit stddev gaussian noise throughout the network
- mathematically you want to predict variance of x @ w
- divide by sqrt of the fan-in w=torch.randn(10,200) /10**.5

- Kaiming He: for relu the factor is (2/10)**.5

- torch.nn.init.kaiming_normal_
  has argument for non-linearity

- 7 years ago you had to be careful with init, was very fragile
- meanwhile we have some innovations that help:
  - residual (connections?)
  - normalization layers (batch, layer, root?)
  - better optimizer (armesprob, adam)

- https://www.youtube.com/watch?v=PaCmpygFfXo
  The spelled-out intro to language modeling: building makemore
- https://github.com/karpathy/makemore
- neural network that creates strings that sound like names but are
  not names

- install dependencies
#+begin_example
pip3 install --user torch
# 777MB
#+end_example

- get dataset 30_000 names from a government website

#+begin_example
cd source
wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt
#+end_example 


* Part 3

- https://www.youtube.com/watch?v=P6sfmUTpUmc
- Building makemore Part 3: Activations & Gradients, BatchNorm

- when dealing with logits make sure that after initialization all the
  outputs are roughly equal in probability. you don't want to init the
  network to be confidently wrong. this will lead to a high loss and
  you have a hockey stick learning curve. you waste cycles during
  training to fix your bad initialization

- leaky relu doesn't make gradients zero. other activation functions
  have horizontal parts where the gradient disappears during
  backprop. this can lead to 'brain-dead' neurons that don't change in
  the training.
- to fix this, make sure during initialization that 

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>FAQ - Video Summarizer & Map</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 20px;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
            color: #333;
            background-color: #fcfcfc;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #bdc3c7;
            padding-bottom: 10px;
            text-align: center;
        }
        h3 {
            color: #2c3e50;
            margin-top: 40px;
            border-left: 5px solid #3498db;
            padding-left: 15px;
            background-color: #f8f9fa;
            padding-top: 10px;
            padding-bottom: 10px;
        }
        dl {
            padding-left: 10px;
        }
        dt {
            font-weight: bold;
            color: #2980b9;
            margin-top: 20px;
            margin-bottom: 8px;
            font-size: 1.1em;
        }
        dd {
            margin-left: 20px;
            margin-bottom: 20px;
            color: #444;
        }
        .note {
            font-style: italic;
            color: #7f8c8d;
            margin-top: 10px;
            display: block;
            font-size: 0.9em;
            background: #f1f1f1;
            padding: 10px;
            border-radius: 4px;
        }
        .cost-list li {
            margin-bottom: 8px;
        }
        code {
            background-color: #eee;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
        }
        .alert {
            background-color: #fff3cd;
            border: 1px solid #ffeeba;
            color: #856404;
            padding: 15px;
            border-radius: 4px;
            margin-top: 20px;
        }
    </style>
</head>
<body>

<h1>Frequently Asked Questions (FAQ)</h1>

<section id="about-service">
    <h3>About the Service & Technology</h3>
    <dl>
        <dt>Q: What is the main purpose of this service?</dt>
        <dd>A: While this tool excels at creating concise summaries of technical lectures and research, its primary goal is to build a <strong>personal map of content consumption</strong>. By visualizing every video I watch, I can reclaim control from recommendation algorithms. I want to see the "local neighborhood" of topics for every video to discover related content organically.
            <span class="note"><strong>Map Status (Oct 2025):</strong> The current map is a manually generated UMAP plot of the first 4,000 entries. Now that the database has reached 8,000 entries, a regeneration is planned. You can view the  example here: <a href="https://rocketrecap.com/exports/index.html">rocketrecap.com/exports/index.html</a></span></dd>

        <dt>Q: Why do I get the error: "Transcript is too short. Probably I couldn't download it"?</dt>
        <dd>
            This error occurs when the system fails to retrieve a usable text transcript for the video. This usually happens for one of three reasons:
            <ul>
                <li><strong>Technical Blocks:</strong> YouTube occasionally blocks automated download attempts. These issues often resolve themselves within a few days.</li>
                <li><strong>No Captions Available:</strong> The video may not have closed captions or an auto-generated transcript enabled. (I am working on integrating <em>Whisper</em> to handle these cases via audio-to-text, but it is not yet live).</li>
                <li><strong>Language Compatibility:</strong> The video might be in a language not currently prioritized by the automated download function (e.g., Hindi).</li>
            </ul>
            <strong>The Workaround:</strong> If the video does have a transcript on YouTube, you can manually copy and paste the text into RocketRecap. This feature works best on desktop browsers.
        </dd>

        <dt>Q: Who pays for this?</dt>
        <dd>A: I built this primarily for my own use. To keep it free for others, I utilize Google’s free developer tier. This provides a shared daily quota (<strong>currently 20 requests for Flash models</strong>) which is distributed among all visitors to the site.</dd>

        <dt>Q: How does the AI "watch" the video from just a link?</dt>
        <dd>A: For YouTube links, the system uses <code>yt-dlp</code> to fetch the captions/transcript. This text is then processed by a Large Language Model (LLM). For specific cases (like some Chinese videos), I manually process the audio using <code>whisper.cpp</code> to generate a transcript, though this isn't fully automated on the site yet.</dd>

        <dt>Q: What are some advanced ways to use this tool?</dt>
        <dd>A: I often use it for specialized tasks:
            <ul>
                <li><strong>Technical Glossaries:</strong> Asking the AI to "provide a glossary of medical/technical terms" from a lecture.</li>
                <li><strong>Informed Summaries:</strong> Manually pasting scientific paper links or YouTube comments into the prompt to give the AI more context.</li>
                <li><strong>Research Clusters:</strong> Using LLM embeddings to group related videos—a technique with great potential in fields like immune system research or genomic sequencing.</li>
            </ul>
        </dd>

        <dt>Q: Why doesn't YouTube provide these summaries itself?</dt>
        <dd>A: YouTube does offer basic summaries for YouTube Premium subscribers. However, those are typically brief "teasers" designed to get you to watch the video. My goal is to create <strong>self-contained summaries</strong> that allow you to absorb the core information without necessarily watching the full video—something YouTube is naturally disincentivized to provide for ad-supported content.</dd>
    </dl>
</section>

<section id="performance-cost">
    <h3>Performance and Cost</h3>
    <dl>
        <dt>Q: How much energy does generating a summary consume?</dt>
        <dd>A: While exact metrics are difficult to track, the cost of API tokens is a good proxy for energy use. Currently, generating a text summary is significantly cheaper (and likely more energy-efficient) than streaming high-definition video for an hour. I am looking into more precise ways to analyze this "digital footprint."</dd>

        <dt>Q: What are the costs for different AI models?</dt>
        <dd>Approximate pricing per million tokens:
            <ul class="cost-list">
                <li><strong>Flash Lite:</strong> $0.10 (Input) / $0.40 (Output)</li>
                <li><strong>Flash:</strong> $0.30 (Input) / $2.50 (Output)</li>
                <li><strong>Pro:</strong> $1.25 (Input) / $10.00 (Output)</li>
            </ul>
        </dd>

        <dt>Q: What are the current usage limits?</dt>
        <dd>
            The service relies on a free developer quota.
            <div class="alert">
                <strong>Current Status (2025-12-06):</strong> Google has significantly reduced the free tier. There are currently 0 "Pro" requests available and only <strong>20 Flash requests per day</strong> for the entire site.
            </div>
        </dd>
    </dl>
</section>

<section id="limitations-future">
    <h3>Limitations and Future Work</h3>
    <dl>
        <dt>Q: Why are some summaries factually incorrect?</dt>
        <dd>AI "hallucinations" or errors usually stem from:
            <ol>
                <li><strong>Knowledge Cutoffs:</strong> The model might not be aware of very recent events (e.g., refering to a current world leader as "hypothetical").</li>
                <li><strong>Transcript Limitations:</strong> Current transcripts lack "speaker diarization" (knowing who is speaking), which can confuse the AI during multi-person interviews or debates.</li>
            </ol>
        </dd>

        <dt>Q: What is on the roadmap?</dt>
        <dd>
            <ul>
                <li><strong>Live Grounding:</strong> Integrating Google Search to verify facts in real-time.</li>
                <li><strong>Auto-Translation:</strong> Support for Spanish and Chinese videos (including platforms like Bilibili).</li>
                <li><strong>Interactive Elements:</strong> Highlighting insightful YouTube comments or corrections made by the community.</li>
            </ul>
        </dd>

        <dt>Q: Is there a plan to commercialize?</dt>
        <dd>No. This is a personal project used to explore the capabilities of LLMs (specifically the Gemini family) and to help me organize my own digital learning.</dd>
    </dl>
</section>

</body>
</html>
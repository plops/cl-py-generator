## Frequently Asked Questions (FAQ) about the Video Summarizer & Map

### About the Service & Technology

**Q: What is the main purpose of this service?**
**A:** This service creates concise summaries from technical lectures and video descriptions. A primary goal is to build a **map of watched videos**. The vision for this feature is to help users gain more control over their content consumption by showing the "local neighborhood" for each video and revealing related content. Please note that the mapping feature is still under development and not yet available on the website.

**Q: Who pays for this?**
**A:** The project is personally maintained and operates on a free daily developer quota from Google (currently **50 Pro** and **5,000 Flash** requests). This quota is shared among all users of the website.

**Q: How does the AI know the content of the video if I only provide a link?**
**A:** For YouTube links, the system uses tools like `yt-dlp` to download available captions or transcripts. This text is then passed to the Large Language Model (LLM) for summarization. If no transcript is available, it's technically possible to extract audio and transcribe it (e.g., with `whisper.cpp`), but this feature is not yet implemented on the website.

**Q: What are some interesting applications beyond simple video summaries?**
**A:** You can ask the AI to generate a glossary of technical terms from a lecture, which is useful for medical or biology content (e.g., from the MicrobeTV channel). For video podcasts, you can include references from the video description in the prompt for a more informed summary. It's also possible to add YouTube comments or live chat history to the transcript to capture audience discussions, though this requires manual copy-pasting for now. The mapping feature, once implemented, will also support research-oriented workflows like literature discovery.

**Q: What makes this service useful for long technical lectures (1-2 hours)?**
**A:** Summaries with timestamps let you quickly grasp key points without watching the entire video. For complex technical lectures, a summary can help ensure you don't miss crucial information. Modern, affordable models like the `flash` family make summarizing content up to an hour long practical and cost-effective.

**Q: Why doesn't YouTube provide detailed summaries?**
**A:** While YouTube is experimenting with AI-generated summaries for Premium users, they tend to be brief outlines. This service aims to produce more detailed, self-contained summaries suitable for technical content. Platforms that rely on ad revenue are generally incentivized to have users watch videos, not just read summaries.

### Performance and Cost

**Q: How much energy/cost does running these summaries require?**
**A:** The cost of generating a summary is used as a proxy for its energy consumption. While a precise comparison is difficult, generating a summary is now considerably cheaper than a year ago and may consume less energy than streaming an entire video.

**Q: How do the costs for different models compare?**
**A:** Approximate pricing for the available models is as follows:
*   **Flash Lite:** ~$0.10 per million input tokens / ~$0.40 per million output tokens.
*   **Flash:** ~$0.30 per million input tokens / ~$2.50 per million output tokens.
*   **Pro:** ~$1.25 per million input tokens / ~$10.00 per million output tokens.
An optional grounding feature using Google Search, which is not enabled on the site, is significantly more expensive at around **$35 per 1,000 prompts**.

**Q: What is the difference between training and inference costs for LLMs?**
**A:** Training a large LLM is an extremely expensive, one-time process. This website only incurs **inference** costsâ€”the much lower cost of running the pre-trained model to generate a summary. The goal is for the societal benefit of easily discoverable knowledge to offset the environmental costs of AI. This works best when summaries are read by people, rather than being generated by automated processes that no one sees.

**Q: What are the current usage quotas?**
**A:** The service relies on a free developer quota, currently limited to **50 Pro** and **5,000 Flash** requests per day. Hitting these limits can cause service interruptions. The quota reset time is not fixed, and there is currently no public counter to track remaining requests.

### Limitations and Future Work

**Q: Why are some generated summaries factually incorrect or confusing?**
**A:** There are two main causes for inaccuracies:
1.  **Outdated Knowledge:** The base LLM's knowledge may be outdated (e.g., it might not be aware of recent events or figures).
2.  **Transcript Quality:** Automatically generated transcripts often contain errors or lack speaker attribution, which can confuse the AI and lead to a muddled summary.

**Q: What improvements are planned for the future?**
**A:** Planned enhancements include:
*   Optional grounding with Google Search for more up-to-date and verifiable information.
*   Automatic **translation** of summaries to and from other languages (e.g., Spanish, Chinese) and support for other platforms like Bilibili.
*   A dedicated feature to generate a **glossary** from a video, which is currently possible via specific prompting.
*   Integration of YouTube comments and live chat to enrich summaries with audience interaction.

**Q: Is there a plan to commercialize the website?**
**A:** No, there are currently no plans for commercialization. This project serves as a personal tool for learning and experimenting with AI capabilities, particularly the Gemini models.

{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "#|default_exp p01_makemore5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "import os\n",
        "import time\n",
        "import pathlib\n",
        "import random\n",
        "import tqdm\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from matplotlib.pyplot import plot, imshow, tight_layout, xlabel, ylabel, title, subplot, subplot2grid, grid, text, legend, figure, gcf, xlim, ylim\n",
        "from torch import linspace, randn, randint, tanh\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# This code trains a network with 20k parameters that generates character sequences that look like names.\n",
        "# Based on the youtube video https://youtu.be/t3YJ5hKiMQ0 0:00 to 11:36 that explains this notebook: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.verbose=True\n",
        "args=Args()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "start_time=time.time()\n",
        "debug=True\n",
        "_code_git_version=\"b97a6a9f14ca89c160742bc0a73b61eb86561792\"\n",
        "_code_repository=\"https://github.com/plops/cl-py-generator/tree/master/example/132_makemore5/source/\"\n",
        "_code_generation_time=\"08:34:57 of Sunday, 2024-05-12 (GMT+1)\"\n",
        "start_time=time.time()\n",
        "debug=True\n",
        "def lprint(msg, args):\n",
        "    if ( args.verbose ):\n",
        "        print(\"{} {}\".format(((time.time())-(start_time)), msg))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "parser=argparse.ArgumentParser()\n",
        "parser.add_argument(\"-v\", \"--verbose\", help=\"enable verbose output\", action=\"store_true\")\n",
        "args=parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# read in all the words\n",
        "words=open(\"names.txt\", \"r\").read().splitlines()\n",
        "lprint(\"len(words)={} max(len(w) for w in words)={}\".format(len(words), max(len(w) for w in words)), args)\n",
        "words[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars=sorted(list(set(\"\".join(words))))\n",
        "# Create a mapping from character to integer (stoi)\n",
        "# Start from 1 because 0 is reserved for the end character '.'\n",
        "stoi={s:((i)+(1)) for i, s in enumerate(chars)}\n",
        "stoi[\".\"]=0\n",
        "# Create a mapping from integer to character (itos)\n",
        "itos={i:s for s, i in stoi.items()}\n",
        "vocab_size=len(itos)\n",
        "lprint(\"mapping from integer to character itos={} vocab_size={}\".format(itos, vocab_size), args)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# shuffle up the words\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# build the dataset\n",
        "# block_size .. context length of how many characters do we take to predict the next one\n",
        "block_size=3\n",
        "def build_dataset(words):\n",
        "    \"\"\"This function builds a dataset for training a model using the given list of words.\n",
        "    It creates a context of a certain block size for each character in the words and uses this to predict the next character.\n",
        "\n",
        "    Args:\n",
        "        words (list): A list of words to be used for creating the dataset.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the input tensor (X) and the target tensor (Y). X is the tensor representing the context for each character, and Y is the tensor representing the characters themselves.\"\"\"\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for w in words:\n",
        "        context=(([0])*(block_size))\n",
        "        for ch in ((w)+(\".\")):\n",
        "            # The character ch is converted to an integer index ix using the stoi function.\n",
        "            ix=stoi[ch]\n",
        "            # The current context is appended to X, and the integer index ix is appended to Y.\n",
        "# \n",
        "# The context is updated by removing the first element and appending the integer index ix at the end. This means that the context always contains the integer indices of the last block_size characters.\n",
        "            X.append(context)\n",
        "            Y.append(ix)\n",
        "            context=((context[1:])+([ix]))\n",
        "    X=torch.tensor(X)\n",
        "    Y=torch.tensor(Y)\n",
        "    # Each element in Y is the character that should be predicted given the corresponding context in X.\n",
        "    lprint(\"X.shape={} Y.shape={}\".format(X.shape, Y.shape), args)\n",
        "    return X, Y\n",
        "# Use 80% for training, 10% for validation and 10% for testing. We use the following indices to perform the split.\n",
        "n1=int((((0.80    ))*(len(words))))\n",
        "n2=int((((0.90    ))*(len(words))))\n",
        "# Training 80%\n",
        "Xtr, Ytr=build_dataset(words[:n1])\n",
        "# Validation 10%\n",
        "Xdev, Ydev=build_dataset(words[n1:n2])\n",
        "# Test 10%\n",
        "Xte, Yte=build_dataset(words[n2:])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for x, y in zip(Xtr[:20], Ytr[:20]):\n",
        "    print(\"\".join((itos[ix.item()] for ix in x)), \"-->\", itos[y.item()])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "class Linear():\n",
        "    \"\"\"A class representing a linear layer in a neural network. It computes a\n",
        "matrix multiplication in the forward pass.\n",
        "\n",
        "    Args:\n",
        "        fan_in (int): The number of input features.\n",
        "        fan_out (int): The number of output features.\n",
        "        bias (bool, optional): Whether to include a bias term. Defaults to True.\n",
        "\"\"\"\n",
        "    def __init__(self, fan_in, fan_out, bias = True):\n",
        "        \"\"\" Initialize the linear layer with weights and bias.\n",
        "\n",
        "        The weights are initialized using Kaiming initialization,\n",
        "        which is a method of initializing neural networks to help\n",
        "        ensure the signal from the input data does not vanish or\n",
        "        explode as it is propagated through the network.\n",
        "\n",
        "        Args:\n",
        "            fan_in (int): The number of input features.\n",
        "            fan_out (int): The number of output features.\n",
        "            bias (bool, optional): Whether to include a bias term. Defaults to True.\n",
        "        \"\"\"\n",
        "        # note: Kaiming init\n",
        "        self.weight=((torch.randn((fan_in,fan_out,)))/(((fan_in)**((0.50    )))))\n",
        "        self.bias=(torch.zeros(fan_out)) if (bias) else (None)\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass through the layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor.\"\"\"\n",
        "        self.out=((x)@(self.weight))\n",
        "        if ( not((self.bias is None)) ):\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        \"\"\"Get the parameters of the layer.\n",
        "\n",
        "        Returns:\n",
        "            list: A list containing the weight tensor and, if it exists, the bias tensor.\n",
        "        \"\"\"\n",
        "        return (([self.weight])+((([]) if ((self.bias is None)) else ([self.bias]))))\n",
        "class BatchNorm1d():\n",
        "    \"\"\"    A class representing a 1-dimensional batch normalization layer.\n",
        "\n",
        "    Batch normalization is a technique for improving the speed,\n",
        "    performance, and stability of neural networks.  It normalizes the\n",
        "    input features across the mini-batch dimension, i.e., for each\n",
        "    feature, it subtracts the mean and divides by the standard\n",
        "    deviation, where both statistics are computed over the mini-batch.\n",
        "    \n",
        "\n",
        "    Note: The BatchNorm1d layer has different behaviors during\n",
        "    training and inference.  It's crucial to set the correct\n",
        "    mode (training or inference) to avoid unexpected results or bugs.\n",
        "    There is state in this layer and state is (usually) harmful.\n",
        "\n",
        "    Note: In BatchNorm1d, the batch dimension serves a specific\n",
        "    purpose beyond efficiency.  It couples computations across batch\n",
        "    elements to control activation statistics, which is integral to\n",
        "    its functionality. \n",
        "\n",
        "    Args:\n",
        "        dim (int): The number of features in the input.\n",
        "        eps (float, optional): A small number added to the denominator for numerical stability. Defaults to 1e-5.\n",
        "        momentum (float, optional): The momentum factor for the running mean and variance computation. Defaults to 0.1\"\"\"\n",
        "    def __init__(self, dim, eps = (1.00e-5), momentum = (0.10    )):\n",
        "        \"\"\" Initialize the batch normalization layer with parameters and buffers.\n",
        "\n",
        "        Args:\n",
        "            dim (int): The number of features in the input.\n",
        "            eps (float, optional): A small number added to the denominator for numerical stability. Defaults to 1e-5.\n",
        "            momentum (float, optional): The momentum factor for the running mean and variance computation. Defaults to 0.1.\n",
        "        \"\"\"\n",
        "        self.eps=eps\n",
        "        self.momentum=momentum\n",
        "        self.training=True\n",
        "        # Parameters (trained with backpropagation)\n",
        "        self.gamma=torch.ones(dim)\n",
        "        self.beta=torch.zeros(dim)\n",
        "        # Buffers (updated with a running 'momentum update')\n",
        "        self.running_mean=torch.zeros(dim)\n",
        "        self.running_var=torch.ones(dim)\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass through the layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor.\"\"\"\n",
        "        if ( self.training ):\n",
        "            xmean=x.mean(0, keepdim=True)\n",
        "            xvar=x.var(0, keepdim=True)\n",
        "        else:\n",
        "            xmean=self.running_mean\n",
        "            xvar=self.running_var\n",
        "        # Normalize to unit variance\n",
        "        xhat=((((x)-(xmean)))/(torch.sqrt(((xvar)+(self.eps)))))\n",
        "        self.out=((((self.gamma)*(xhat)))+(self.beta))\n",
        "        # Update the buffers\n",
        "        if ( self.training ):\n",
        "            with torch.no_grad():\n",
        "                self.running_mean=((((((1)-(self.momentum)))*(self.running_mean)))+(((self.momentum)*(xmean))))\n",
        "                self.running_var=((((((1)-(self.momentum)))*(self.running_var)))+(((self.momentum)*(xvar))))\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        \"\"\"Get the parameters of the layer.\n",
        "Returns:\n",
        "            list: A list containing the gamma and beta tensors.\"\"\"\n",
        "        return [self.gamma, self.beta]\n",
        "class Tanh():\n",
        "    \"\"\"A class representing the hyperbolic tangent activation function.\n",
        "\n",
        "    The hyperbolic tangent function, or tanh, is a function that squashes its input into the range between -1 and 1.\n",
        "    It is commonly used as an activation function in neural networks.\"\"\"\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Apply the tanh function to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor, where the tanh function has been applied element-wise.\n",
        "        \"\"\"\n",
        "        self.out=torch.tanh(x)\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        \"\"\"Get the parameters of the layer.\n",
        "\n",
        "        The tanh function does not have any parameters, so this method returns an empty list.\n",
        "\n",
        "        Returns:\n",
        "            list: An empty list.\"\"\"\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# Seed rng for reproducibility\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# The dimensionality of the character embedding vectors.\n",
        "n_embed=10\n",
        "# The number of neurons in the hidden layer of the MLP\n",
        "n_hidden=200\n",
        "# Create the embedding table C:\n",
        "# C is a matrix where each row represents a character in the vocabulary,\n",
        "# and each column represents a dimension in the embedding space.\n",
        "C=torch.randn((vocab_size,n_embed,))\n",
        "# Define the list of layers\n",
        "# The MLP consists of a linear layer, a batch normalization layer, a\n",
        "# tanh activation function, and another linear layer. The output of the\n",
        "# MLP is a probability distribution over the vocabulary.\n",
        "layers=[Linear(((n_embed)*(block_size)), n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(), Linear(n_hidden, vocab_size)]\n",
        "# Make the last layer less confident. This is done by scaling down the\n",
        "# weights of the last layer. This can help for the network to be initially overconfidently wrong.\n",
        "with torch.no_grad():\n",
        "    layers[-1].weight=(((0.10    ))*(layers[-1].weight))\n",
        "# Gather all the parameters of the model: This includes the embedding\n",
        "# table C and the parameters of all the layers in the MLP.\n",
        "parameters=(([C])+([p for layer in layers for p in layer.parameters()]))\n",
        "lprint(\"Number of parameters in total sum(p.nelement() for p in parameters)={}\".format(sum(p.nelement() for p in parameters)), args)\n",
        "for p in parameters:\n",
        "    p.requires_grad=True\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# Maximum number of training steps\n",
        "max_steps=200_000\n",
        "# Size of the minibatches\n",
        "batch_size=32\n",
        "# List to store the loss values\n",
        "lossi=[]\n",
        "# Start the training loop\n",
        "for i in range(max_steps):\n",
        "    # Construct a minibatch. Xb holds input data, Yb the corresponding target data\n",
        "    ix=torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "    Xb=Xtr[ix]\n",
        "    Yb=Ytr[ix]\n",
        "    # Forward pass\n",
        "    # Embed the input data into vectors\n",
        "    emb=C[Xb]\n",
        "    # Reshape the embedded data\n",
        "    x=emb.view(emb.shape[0], -1)\n",
        "    # Pass the data through each layer\n",
        "    for layer in layers:\n",
        "        x=layer(x)\n",
        "    # Compute the loss (cross entropy)\n",
        "    loss=F.cross_entropy(x, Yb)\n",
        "    # Backward pass\n",
        "    for p in parameters:\n",
        "        # Clear the gradient for each parameter\n",
        "        p.grad=None\n",
        "    # Compute the gradient of the loss with respect to the parameters\n",
        "    loss.backward()\n",
        "    # Update the parameters using simple SGD with step learning rate decay\n",
        "    lr=((0.10    )) if (((i)<(150_000))) else ((1.00e-2))\n",
        "    for p in parameters:\n",
        "        # Update the parameter using its gradient\n",
        "        p.data += ((-lr)*(p.grad))\n",
        "    # Track the progress (every 10k steps)\n",
        "    if ( ((0)==(((i)%(10_000)))) ):\n",
        "        progress=((i)/(max_steps))\n",
        "        lprint(\"progress={} loss.item()={}\".format(progress, loss.item()), args)\n",
        "    # Append the logarithm of the loss to the list\n",
        "    lossi.append(loss.log10().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "plt.plot(lossi)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# average 1000 values into one\n",
        "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# Put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in layers:\n",
        "    layer.training=False\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# Evaluate the loss for a given data split (train, validation, or\n",
        "# test). This function is decorated with @torch.no_grad() to disable\n",
        "# gradient tracking in PyTorch, as we only want to evaluate the loss and\n",
        "# not perform any updates.\n",
        "# \n",
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "    # Select the appropriate data based on the provided split\n",
        "    x, y=dict(train=(Xtr,Ytr,), val=(Xdev,Ydev,), test=(Xte,Yte,))[split]\n",
        "    # (N, block_size, n_embed)\n",
        "    emb=C[x]\n",
        "    # Reshape embedded data into (N, block_size*n_embed)\n",
        "    x=emb.view(emb.shape[0], -1)\n",
        "    # Pass the reshaped data through each layer of the model\n",
        "    for layer in layers:\n",
        "        x=layer(x)\n",
        "    # Compute cross-entropy loss between model's output and the target data\n",
        "    loss=F.cross_entropy(x, y)\n",
        "    # Print the loss for the current split\n",
        "    lprint(\"split={} loss.item()={}\".format(split, loss.item()), args)\n",
        "# Evaluate and print the loss for the training and validation splits\n",
        "split_loss(\"train\")\n",
        "split_loss(\"val\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# Generate 20 words using the trained model\n",
        "for _ in range(20):\n",
        "    # List to store the generated characters\n",
        "    out=[]\n",
        "    # Initialize context with all end character '.' represented by 0\n",
        "    context=(([0])*(block_size))\n",
        "    while (True):\n",
        "        # Forward pass through the the neural net\n",
        "        # 1 block_size n_embed\n",
        "        emb=C[torch.tensor([context])]\n",
        "        x=emb.view(emb.shape[0], -1)\n",
        "        for layer in layers:\n",
        "            x=layer(x)\n",
        "        logits=x\n",
        "        # Compute the softmax probabilities from the output logits\n",
        "        probs=F.softmax(logits, dim=1)\n",
        "        # Sample the character from the softmax distribution\n",
        "        ix=torch.multinomial(probs, num_samples=1).item()\n",
        "        # Update the context by removing the first character and appending the sampled character\n",
        "        context=((context[1:])+([ix]))\n",
        "        # Add the sampled character to the output list\n",
        "        out.append(ix)\n",
        "        # Break the loop if we sample the special '.' token represented by 0\n",
        "        if ( ((ix)==(0)) ):\n",
        "            break\n",
        "    # Decode and print the generated word\n",
        "    print(\"\".join(itos[i] for i in out))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# |export\n",
        "# the video explains from 11:36 to 18:00 how to make the code simpler by introducing additional abstraction layers. https://youtu.be/t3YJ5hKiMQ0?t=696\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
